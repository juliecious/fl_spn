{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-28T11:36:21.293714Z",
     "start_time": "2025-08-28T11:36:21.291415Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from simple_einet.einet import Einet, EinetConfig\n",
    "from simple_einet.layers.distributions.piecewise_linear import PiecewiseLinear\n",
    "from simple_einet.dist import DataType, Domain"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Import, Preprocess and Split the Dataset \n",
    "\n",
    "Traditional non-federated learning mode"
   ],
   "id": "32fde84733b0454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:34:44.667443Z",
     "start_time": "2025-08-28T13:34:44.582326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = fetch_openml('adult', version=2, as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "df = df.replace('?', np.nan)\n",
    "df_clean = df.dropna()\n",
    "X = df_clean.drop('class', axis=1)\n",
    "y = df_clean['class']\n"
   ],
   "id": "85c54b159b178d05",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:34:46.177918Z",
     "start_time": "2025-08-28T13:34:46.174531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical feat: ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical feat: ({len(categorical_features)}): {categorical_features}\")"
   ],
   "id": "be3d3b302abe2e80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical feat: (6): ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Categorical feat: (8): ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:35:18.748634Z",
     "start_time": "2025-08-28T11:35:18.661004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', LabelEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "X_numeric = StandardScaler().fit_transform(X[numeric_features])\n",
    "X_numeric_df = pd.DataFrame(X_numeric, columns=numeric_features, index=X.index)\n",
    "\n",
    "X_categorical_encoded = pd.DataFrame(index=X.index)\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_categorical_encoded[col] = le.fit_transform(X[col].astype(str))\n",
    "    \n",
    "X_processed = pd.concat([X_numeric_df, X_categorical_encoded], axis=1)\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "print(f\"X shape after preprocessed: {X_processed.shape}\")\n",
    "print(f\"Target unique: {np.unique(y_encoded)}\")"
   ],
   "id": "f743203e4dff30ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape after preprocessed: (45222, 14)\n",
      "Target unique: [0 1]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:35:42.249416Z",
     "start_time": "2025-08-28T11:35:42.228661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed.values, y_encoded, test_size=0.33, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "print(f\"X Train shape: {X_train_tensor.shape}\")\n",
    "print(f\"X Test shape: {X_test_tensor.shape}\")"
   ],
   "id": "5dcbb518a0c83617",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape: torch.Size([30298, 14])\n",
      "X Test shape: torch.Size([14924, 14])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Construct the domain used for Einet with Piecewise Distribution",
   "id": "dc321f3d87cb6ca8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:37:24.022536Z",
     "start_time": "2025-08-28T11:37:24.018251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "domains = []\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "for i, feature in enumerate(all_features):\n",
    "    if feature in numeric_features:\n",
    "        # æ•¸å€¼å‹ç‰¹å¾µä½¿ç”¨é€£çºŒåŸŸ\n",
    "        domains.append(Domain(data_type=DataType.CONTINUOUS))\n",
    "    else:\n",
    "        # é¡åˆ¥å‹ç‰¹å¾µä½¿ç”¨é›¢æ•£åŸŸ\n",
    "        # ç²å–å”¯ä¸€å€¼ä½œç‚ºé›¢æ•£å€¼åŸŸ\n",
    "        unique_values = sorted(X_processed[feature].unique())\n",
    "        domains.append(Domain(data_type=DataType.DISCRETE, values=unique_values))\n",
    "\n",
    "print(f\"Defined {len(domains)} feature domains.\")"
   ],
   "id": "3410b08c067a53d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 14 feature domains.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configure Einet",
   "id": "cf6c7182cb8ce924"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:44:38.442259Z",
     "start_time": "2025-08-28T11:44:38.437176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# é‡å¡‘è¨“ç·´è³‡æ–™ç‚º EiNet æ‰€éœ€çš„æ ¼å¼ [batch_size, channels, features]\n",
    "# é€™è£¡ channels=1ï¼Œå› ç‚ºæˆ‘å€‘æ²’æœ‰å¤šé€šé“è³‡æ–™\n",
    "X_train_reshaped = X_train_tensor.unsqueeze(1)  # [batch_size, 1, features]\n",
    "\n",
    "# é…ç½® EiNet ä½¿ç”¨ PiecewiseLinear åˆ†å¸ƒ\n",
    "config = EinetConfig(\n",
    "    num_features=X_train_tensor.shape[1],  # ç‰¹å¾µæ•¸é‡\n",
    "    depth=2,  # ç¶²è·¯æ·±åº¦\n",
    "    num_sums=10,  # sum nodes æ•¸é‡\n",
    "    num_leaves=10,  # leaf nodes æ•¸é‡  \n",
    "    num_repetitions=5,  # é‡è¤‡æ•¸é‡\n",
    "    num_classes=2,  # åˆ†é¡é¡åˆ¥æ•¸ï¼ˆ<=50K, >50Kï¼‰\n",
    "    leaf_type=PiecewiseLinear,  # ä½¿ç”¨ PiecewiseLinear åˆ†å¸ƒ\n",
    "    leaf_kwargs={'alpha': 0.1},  # Laplace å¹³æ»‘åƒæ•¸\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "model = Einet(config)\n",
    "print(f\"æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters())}\")"
   ],
   "id": "ab20438e31edb91f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åƒæ•¸æ•¸é‡: 1119\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T11:44:50.238767Z",
     "start_time": "2025-08-28T11:44:49.631879Z"
    }
   },
   "cell_type": "code",
   "source": "model.leaf.base_leaf.initialize(X_train_reshaped, domains)",
   "id": "382ca27e7db14426",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing PiecewiseLinear Leaf Layer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  8.41it/s]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the model",
   "id": "27a7b48db94d88f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:04:00.385991Z",
     "start_time": "2025-08-28T19:03:55.902262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.argmax(-1) \n",
    "        correct = (predictions == y).sum()\n",
    "        total = y.shape[0]\n",
    "        return 100. * correct / total\n",
    "\n",
    "\n",
    "def f1(model, X, y, num_classes=None):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.argmax(-1)\n",
    "        # å‡è¨­ y å’Œ predictions ç‚º 1D tensor\n",
    "        if num_classes is None:\n",
    "            num_classes = int(torch.max(y).item()) + 1\n",
    "        f1_scores = []\n",
    "        for c in range(num_classes):\n",
    "            tp = ((predictions == c) & (y == c)).sum().item()\n",
    "            fp = ((predictions == c) & (y != c)).sum().item()\n",
    "            fn = ((predictions != c) & (y == c)).sum().item()\n",
    "            if tp + fp + fn == 0:\n",
    "                f1 = 0.0  # é˜²æ­¢0é™¤\n",
    "            else:\n",
    "                precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "                if precision + recall == 0:\n",
    "                    f1 = 0.0\n",
    "                else:\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            f1_scores.append(f1)\n",
    "        # å– macro-average F1\n",
    "        return 100. * sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "\n",
    "X_test_reshaped = X_test_tensor.unsqueeze(1)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"Start training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    log_likelihoods = model(X_train_reshaped)\n",
    "    loss = cross_entropy(log_likelihoods, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        acc_train = accuracy(model, X_train_reshaped, y_train_tensor)\n",
    "        acc_test = accuracy(model, X_test_reshaped, y_test_tensor)\n",
    "        print(f\"Epoch: {epoch+1:2d}, Loss: {loss.item():.4f}, \"\n",
    "              f\"Train Acc: {acc_train:.2f}%, Test Acc: {acc_test:.2f}%\")\n",
    "\n",
    "print(\"Finished trainingï¼\")"
   ],
   "id": "8f2638bd26999126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch:  5, Loss: 0.4959, Train Acc: 64.07%, Test Acc: 62.89%\n",
      "Epoch: 10, Loss: 0.4865, Train Acc: 64.86%, Test Acc: 64.05%\n",
      "Finished trainingï¼\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Going Federated by constructing a Data Partitioner for \n",
    "- Horizontal \n",
    "- Vertical\n",
    "- Hybrid "
   ],
   "id": "6cfaacb3075bcab7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T13:33:55.616795Z",
     "start_time": "2025-08-28T13:33:55.612297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from typing import Dict, List"
   ],
   "id": "ef09254762fcf4b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T15:11:22.506041Z",
     "start_time": "2025-08-28T15:11:22.485120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å¥å£¯ç‰ˆè¯é‚¦å­¸ç¿’è³‡æ–™åˆ†å‰²å™¨ï¼ˆèˆ‡ä¹‹å‰ç›¸åŒï¼‰\n",
    "class FederatedDataPartitionerRobust:\n",
    "    \"\"\"å¥å£¯ç‰ˆè¯é‚¦å­¸ç¿’è³‡æ–™åˆ†å‰²å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, feature_names, numeric_features, categorical_features):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = feature_names\n",
    "        self.numeric_features = numeric_features\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def horizontal_partition(self, num_clients: int = 3, random_state: int = 42) -> Dict:\n",
    "        \"\"\"æ°´å¹³åˆ†å‰²ï¼šç›¸åŒç‰¹å¾µï¼Œä¸åŒæ¨£æœ¬\"\"\"\n",
    "        print(f\"ğŸ”„ åŸ·è¡Œæ°´å¹³åˆ†å‰²ï¼Œåˆ†æˆ {num_clients} å€‹å®¢æˆ¶ç«¯...\")\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        n_samples = len(self.X)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        clients = {}\n",
    "        samples_per_client = n_samples // num_clients\n",
    "        \n",
    "        for i in range(num_clients):\n",
    "            start_idx = i * samples_per_client\n",
    "            if i == num_clients - 1:\n",
    "                end_idx = n_samples\n",
    "            else:\n",
    "                end_idx = (i + 1) * samples_per_client\n",
    "                \n",
    "            client_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            clients[f'client_{i}'] = {\n",
    "                'X': self.X[client_indices],\n",
    "                'y': self.y[client_indices],\n",
    "                'features': self.feature_names,\n",
    "                'numeric_features': self.numeric_features,\n",
    "                'categorical_features': self.categorical_features,\n",
    "                'n_samples': len(client_indices),\n",
    "                'n_features': len(self.feature_names),\n",
    "                'sample_indices': client_indices,\n",
    "                'feature_indices': list(range(len(self.feature_names))),\n",
    "                'feature_overlap': self.feature_names  # å®Œå…¨é‡ç–Š\n",
    "            }\n",
    "            \n",
    "            print(f\"  å®¢æˆ¶ç«¯ {i}: {len(client_indices)} æ¨£æœ¬, {len(self.feature_names)} ç‰¹å¾µ\")\n",
    "            \n",
    "        return {\n",
    "            'type': 'horizontal',\n",
    "            'clients': clients,\n",
    "            'total_samples': n_samples,\n",
    "            'total_features': len(self.feature_names)\n",
    "        }\n",
    "    \n",
    "    def vertical_partition(self, num_clients: int = 3, random_state: int = 42) -> Dict:\n",
    "        \"\"\"å‚ç›´åˆ†å‰²ï¼šç›¸åŒæ¨£æœ¬ï¼Œä¸åŒç‰¹å¾µ\"\"\"\n",
    "        print(f\"ğŸ”„ åŸ·è¡Œå‚ç›´åˆ†å‰²ï¼Œåˆ†æˆ {num_clients} å€‹å®¢æˆ¶ç«¯...\")\n",
    "        \n",
    "        random.seed(random_state)\n",
    "        all_features = self.feature_names.copy()\n",
    "        random.shuffle(all_features)\n",
    "        \n",
    "        features_per_client = len(all_features) // num_clients\n",
    "        clients = {}\n",
    "        \n",
    "        for i in range(num_clients):\n",
    "            start_idx = i * features_per_client\n",
    "            if i == num_clients - 1:\n",
    "                end_idx = len(all_features)\n",
    "            else:\n",
    "                end_idx = (i + 1) * features_per_client\n",
    "                \n",
    "            client_features = all_features[start_idx:end_idx]\n",
    "            client_numeric = [f for f in client_features if f in self.numeric_features]\n",
    "            client_categorical = [f for f in client_features if f in self.categorical_features]\n",
    "            \n",
    "            feature_indices = [self.feature_names.index(f) for f in client_features]\n",
    "            \n",
    "            clients[f'client_{i}'] = {\n",
    "                'X': self.X[:, feature_indices],\n",
    "                'y': self.y,\n",
    "                'features': client_features,\n",
    "                'numeric_features': client_numeric,\n",
    "                'categorical_features': client_categorical,\n",
    "                'n_samples': len(self.X),\n",
    "                'n_features': len(client_features),\n",
    "                'feature_indices': feature_indices,\n",
    "                'sample_indices': list(range(len(self.X))),\n",
    "                'feature_overlap': []  # å‚ç›´åˆ†å‰²ç„¡ç‰¹å¾µé‡ç–Š\n",
    "            }\n",
    "            \n",
    "            print(f\"  å®¢æˆ¶ç«¯ {i}: {len(self.X)} æ¨£æœ¬, {len(client_features)} ç‰¹å¾µ\")\n",
    "            \n",
    "        return {\n",
    "            'type': 'vertical', \n",
    "            'clients': clients,\n",
    "            'total_samples': len(self.X),\n",
    "            'total_features': len(self.feature_names)\n",
    "        }\n",
    "    \n",
    "    def hybrid_partition(self, num_clients: int = 4, \n",
    "                        sample_overlap_ratio: float = 0.3,\n",
    "                        feature_overlap_ratio: float = 0.2,\n",
    "                        random_state: int = 42) -> Dict:\n",
    "        \"\"\"å¥å£¯ç‰ˆæ··åˆåˆ†å‰²ï¼šæ­£ç¢ºå¯¦ç¾æ¨£æœ¬å’Œç‰¹å¾µé‡ç–Š\"\"\"\n",
    "        print(f\"ğŸ”„ åŸ·è¡Œå¥å£¯ç‰ˆæ··åˆåˆ†å‰²ï¼Œåˆ†æˆ {num_clients} å€‹å®¢æˆ¶ç«¯...\")\n",
    "        print(f\"  æ¨£æœ¬é‡ç–Šæ¯”ä¾‹: {sample_overlap_ratio:.1%}\")\n",
    "        print(f\"  ç‰¹å¾µé‡ç–Šæ¯”ä¾‹: {feature_overlap_ratio:.1%}\")\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        random.seed(random_state)\n",
    "        \n",
    "        n_samples = len(self.X)\n",
    "        n_features = len(self.feature_names)\n",
    "        \n",
    "        # æ¨£æœ¬åˆ†é…ç­–ç•¥\n",
    "        base_samples_per_client = max(1, int(n_samples * 0.5 / num_clients))\n",
    "        overlap_sample_count = int(n_samples * sample_overlap_ratio)\n",
    "        all_sample_indices = np.arange(n_samples, dtype=int)\n",
    "        np.random.shuffle(all_sample_indices)\n",
    "        \n",
    "        base_samples_end = min(base_samples_per_client * num_clients, n_samples)\n",
    "        base_sample_indices = all_sample_indices[:base_samples_end]\n",
    "        \n",
    "        if overlap_sample_count > 0 and base_samples_end < n_samples:\n",
    "            remaining_samples = all_sample_indices[base_samples_end:]\n",
    "            overlap_sample_indices = remaining_samples[:min(overlap_sample_count, len(remaining_samples))]\n",
    "        else:\n",
    "            overlap_sample_indices = np.array([], dtype=int)\n",
    "        \n",
    "        # ç‰¹å¾µåˆ†é…ç­–ç•¥\n",
    "        base_features_per_client = max(1, int(n_features * 0.6 / num_clients))\n",
    "        overlap_feature_count = int(n_features * feature_overlap_ratio)\n",
    "        all_feature_indices = np.arange(n_features, dtype=int)\n",
    "        np.random.shuffle(all_feature_indices)\n",
    "        \n",
    "        base_features_end = min(base_features_per_client * num_clients, n_features)\n",
    "        base_feature_indices = all_feature_indices[:base_features_end]\n",
    "        \n",
    "        if overlap_feature_count > 0 and base_features_end < n_features:\n",
    "            remaining_features = all_feature_indices[base_features_end:]\n",
    "            overlap_feature_indices = remaining_features[:min(overlap_feature_count, len(remaining_features))]\n",
    "        else:\n",
    "            overlap_feature_indices = np.array([], dtype=int)\n",
    "        \n",
    "        print(f\"  åŸºç¤æ¨£æœ¬: {len(base_sample_indices)} å€‹ï¼Œé‡ç–Šæ¨£æœ¬æ± : {len(overlap_sample_indices)} å€‹\")\n",
    "        print(f\"  åŸºç¤ç‰¹å¾µ: {len(base_feature_indices)} å€‹ï¼Œé‡ç–Šç‰¹å¾µæ± : {len(overlap_feature_indices)} å€‹\")\n",
    "        \n",
    "        # ç‚ºæ¯å€‹å®¢æˆ¶ç«¯åˆ†é…è³‡æ–™\n",
    "        clients = {}\n",
    "        \n",
    "        for i in range(num_clients):\n",
    "            # æ¨£æœ¬åˆ†é…\n",
    "            client_base_start = i * base_samples_per_client\n",
    "            client_base_end = min((i + 1) * base_samples_per_client, len(base_sample_indices))\n",
    "            client_base_samples = base_sample_indices[client_base_start:client_base_end]\n",
    "            \n",
    "            if len(overlap_sample_indices) > 0:\n",
    "                overlap_sample_size = min(len(overlap_sample_indices), len(client_base_samples) // 2)\n",
    "                if overlap_sample_size > 0:\n",
    "                    client_overlap_samples = np.random.choice(\n",
    "                        overlap_sample_indices, size=overlap_sample_size, replace=False\n",
    "                    )\n",
    "                else:\n",
    "                    client_overlap_samples = np.array([], dtype=int)\n",
    "            else:\n",
    "                client_overlap_samples = np.array([], dtype=int)\n",
    "            \n",
    "            if len(client_overlap_samples) > 0:\n",
    "                client_sample_indices = np.concatenate([client_base_samples, client_overlap_samples])\n",
    "            else:\n",
    "                client_sample_indices = client_base_samples.copy()\n",
    "            client_sample_indices = np.unique(client_sample_indices)\n",
    "            \n",
    "            # ç‰¹å¾µåˆ†é…\n",
    "            client_base_feat_start = i * base_features_per_client\n",
    "            client_base_feat_end = min((i + 1) * base_features_per_client, len(base_feature_indices))\n",
    "            client_base_features = base_feature_indices[client_base_feat_start:client_base_feat_end]\n",
    "            \n",
    "            if len(overlap_feature_indices) > 0:\n",
    "                guaranteed_overlap_size = max(1, len(overlap_feature_indices) // 2)\n",
    "                guaranteed_overlap_features = overlap_feature_indices[:guaranteed_overlap_size]\n",
    "                client_overlap_features = guaranteed_overlap_features\n",
    "            else:\n",
    "                client_overlap_features = np.array([], dtype=int)\n",
    "            \n",
    "            if len(client_overlap_features) > 0:\n",
    "                client_feature_indices = np.concatenate([client_base_features, client_overlap_features])\n",
    "            else:\n",
    "                client_feature_indices = client_base_features.copy()\n",
    "            client_feature_indices = np.unique(client_feature_indices.astype(int))\n",
    "            \n",
    "            # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹ç‰¹å¾µ\n",
    "            if len(client_feature_indices) == 0:\n",
    "                client_feature_indices = np.array([0], dtype=int)\n",
    "            \n",
    "            # ç²å–ç‰¹å¾µåç¨±\n",
    "            client_features = [self.feature_names[idx] for idx in client_feature_indices]\n",
    "            client_numeric = [f for f in client_features if f in self.numeric_features]\n",
    "            client_categorical = [f for f in client_features if f in self.categorical_features]\n",
    "            \n",
    "            # è¨ˆç®—é‡ç–Šç‰¹å¾µ\n",
    "            overlap_features_names = []\n",
    "            if len(client_overlap_features) > 0:\n",
    "                overlap_features_names = [self.feature_names[idx] for idx in client_overlap_features]\n",
    "            \n",
    "            clients[f'client_{i}'] = {\n",
    "                'X': self.X[np.ix_(client_sample_indices, client_feature_indices)],\n",
    "                'y': self.y[client_sample_indices],\n",
    "                'features': client_features,\n",
    "                'numeric_features': client_numeric,\n",
    "                'categorical_features': client_categorical,\n",
    "                'n_samples': len(client_sample_indices),\n",
    "                'n_features': len(client_features),\n",
    "                'feature_indices': client_feature_indices,\n",
    "                'sample_indices': client_sample_indices,\n",
    "                'base_sample_count': len(client_base_samples),\n",
    "                'overlap_sample_count': len(client_overlap_samples),\n",
    "                'base_feature_count': len(client_base_features),\n",
    "                'overlap_feature_count': len(client_overlap_features),\n",
    "                'feature_overlap': overlap_features_names\n",
    "            }\n",
    "            \n",
    "            print(f\"  å®¢æˆ¶ç«¯ {i}: {len(client_sample_indices)} æ¨£æœ¬ Ã— {len(client_features)} ç‰¹å¾µ\")\n",
    "        \n",
    "        return {\n",
    "            'type': 'hybrid_robust',\n",
    "            'clients': clients,\n",
    "            'total_samples': n_samples,\n",
    "            'total_features': n_features,\n",
    "            'sample_overlap_ratio': sample_overlap_ratio,\n",
    "            'feature_overlap_ratio': feature_overlap_ratio\n",
    "        }\n"
   ],
   "id": "d4de123a4fe1bd89",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:14:26.446939Z",
     "start_time": "2025-08-28T19:14:26.428777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "\n",
    "def _accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.argmax(-1) \n",
    "        correct = (predictions == y).sum()\n",
    "        total = y.shape[0]\n",
    "        return 100. * correct / total\n",
    "\n",
    "\n",
    "def _f1_score(model, X, y, num_classes=None):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        predictions = outputs.argmax(-1)\n",
    "        # å‡è¨­ y å’Œ predictions ç‚º 1D tensor\n",
    "        if num_classes is None:\n",
    "            num_classes = int(torch.max(y).item()) + 1\n",
    "        f1_scores = []\n",
    "        for c in range(num_classes):\n",
    "            tp = ((predictions == c) & (y == c)).sum().item()\n",
    "            fp = ((predictions == c) & (y != c)).sum().item()\n",
    "            fn = ((predictions != c) & (y == c)).sum().item()\n",
    "            if tp + fp + fn == 0:\n",
    "                f1 = 0.0  # é˜²æ­¢0é™¤\n",
    "            else:\n",
    "                precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "                recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "                if precision + recall == 0:\n",
    "                    f1 = 0.0\n",
    "                else:\n",
    "                    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            f1_scores.append(f1)\n",
    "        # å– macro-average F1\n",
    "        return 100. * sum(f1_scores) / len(f1_scores)\n",
    "    \n",
    "# åŸºæ–¼ simple-einet API çš„è¯é‚¦å­¸ç¿’è¨“ç·´å™¨\n",
    "class FederatedEiNetTrainer:\n",
    "    \"\"\"\n",
    "    è¯é‚¦ EiNet è¨“ç·´å™¨ - ä½¿ç”¨ simple-einet API é¢¨æ ¼\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, partition_info: Dict):\n",
    "        self.partition_info = partition_info\n",
    "        self.client_models = {}\n",
    "        self.client_domains = {}\n",
    "        self.training_history = {}\n",
    "        \n",
    "    def create_domains(self, features: List[str], numeric_features: List[str], \n",
    "                      categorical_features: List[str], X_processed: pd.DataFrame) -> List:\n",
    "        \"\"\"ç‚ºçµ¦å®šç‰¹å¾µå‰µå»º domains\"\"\"\n",
    "        domains = []\n",
    "        \n",
    "        for feature in features:\n",
    "            if feature in numeric_features:\n",
    "                if feature in X_processed.columns:\n",
    "                    min_val = float(X_processed[feature].min())\n",
    "                    max_val = float(X_processed[feature].max())\n",
    "                    domains.append(Domain.continuous_range(min_val, max_val))\n",
    "                else:\n",
    "                    # å¦‚æœç‰¹å¾µä¸åœ¨ X_processed ä¸­ï¼Œä½¿ç”¨é è¨­ç¯„åœ\n",
    "                    domains.append(Domain.continuous_range(-3.0, 3.0))\n",
    "            else:\n",
    "                if feature in X_processed.columns:\n",
    "                    values = sorted(X_processed[feature].unique().tolist())\n",
    "                    domains.append(Domain.discrete_bins(values))\n",
    "                else:\n",
    "                    # å¦‚æœç‰¹å¾µä¸åœ¨ X_processed ä¸­ï¼Œä½¿ç”¨é è¨­å€¼\n",
    "                    domains.append(Domain.discrete_bins([0, 1]))\n",
    "                \n",
    "        return domains\n",
    "    \n",
    "    def train_client(self, client_id: str, client_data: Dict, X_processed: pd.DataFrame,\n",
    "                    epochs: int = 100, verbose: bool = False) -> Dict:\n",
    "        \"\"\"è¨“ç·´å–®å€‹å®¢æˆ¶ç«¯çš„ EiNet æ¨¡å‹\"\"\"\n",
    "        \n",
    "        X_client = client_data['X']\n",
    "        y_client = client_data['y']\n",
    "        \n",
    "        X_client_reshaped = X_client.unsqueeze(1)\n",
    "        \n",
    "        # å‰µå»ºè©²å®¢æˆ¶ç«¯ç‰¹å¾µçš„ domains\n",
    "        domains = self.create_domains(\n",
    "            client_data['features'],\n",
    "            client_data['numeric_features'],\n",
    "            client_data['categorical_features'],\n",
    "            X_processed,\n",
    "        )\n",
    "        \n",
    "        num_features = client_data['n_features']\n",
    "        \n",
    "        # å‹•æ…‹èª¿æ•´æ¨¡å‹è¤‡é›œåº¦\n",
    "        if num_features < 3:\n",
    "            depth, num_sums, num_leaves = 1, 4, 4\n",
    "        elif num_features < 6:\n",
    "            depth, num_sums, num_leaves = 1, 8, 8  \n",
    "        else:\n",
    "            depth, num_sums, num_leaves = 2, 12, 12\n",
    "            \n",
    "        config = EinetConfig(\n",
    "            num_features=num_features,\n",
    "            depth=depth,\n",
    "            num_sums=num_sums,\n",
    "            num_leaves=num_leaves,\n",
    "            num_repetitions=3,\n",
    "            num_classes=2,\n",
    "            leaf_type=PiecewiseLinear,\n",
    "            leaf_kwargs={'alpha': 0.1},\n",
    "            dropout=0.0\n",
    "        )\n",
    "        \n",
    "        model = Einet(config)\n",
    "        model.leaf.base_leaf.initialize(X_client_reshaped, domains)\n",
    "        \n",
    "        cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    ğŸ“Š æ¨¡å‹é…ç½®: depth={depth}, sums={num_sums}, leaves={num_leaves}\")\n",
    "            print(f\"    ğŸ”§ ç‰¹å¾µåŸŸ: {len(domains)} å€‹ domain\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            ll = model(X_client_reshaped)\n",
    "            loss = cross_entropy(ll, y_client)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                acc_train = _accuracy(model, X_client_reshaped, y_client)\n",
    "                f1_train = _f1_score(model, X_client_reshaped, y_client)\n",
    "                print(f\"Epoch: {epoch+1:2d}, Loss: {loss.item():.4f}, \"\n",
    "                      f\"Train Acc: {acc_train:.2f}%, F1: {f1_train:.2f}%\")\n",
    "            \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        train_accuracy = _accuracy(model, X_client_reshaped, y_client)\n",
    "        train_f1 = _f1_score(model, X_client_reshaped, y_client)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    âœ… è¨“ç·´æº–ç¢ºç‡: {train_accuracy:.3f}\")\n",
    "            print(f\"    ğŸ“ˆ è¨“ç·´ F1 åˆ†æ•¸: {train_f1:.3f}\")\n",
    "            print(f\"    â±ï¸  è¨“ç·´æ™‚é–“: {training_time:.3f} ç§’\")\n",
    "        \n",
    "        return {\n",
    "            'client_id': client_id,\n",
    "            'model': model,\n",
    "            'domains': domains,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'train_f1': train_f1,\n",
    "            'training_time': training_time,\n",
    "            'config': config\n",
    "        }\n",
    "    \n",
    "    def train_federated_learning(self, X_processed: pd.DataFrame, epochs: int = 100, \n",
    "                               verbose: bool = True) -> Dict:\n",
    "        \"\"\"åŸ·è¡Œè¯é‚¦å­¸ç¿’è¨“ç·´\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸš€ é–‹å§‹ {self.partition_info['type']} è¯é‚¦å­¸ç¿’è¨“ç·´...\")\n",
    "        print(f\"è¨“ç·´åƒæ•¸: epochs={epochs}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = {}\n",
    "        \n",
    "        # è¨“ç·´æ¯å€‹å®¢æˆ¶ç«¯\n",
    "        for client_id, client_data in self.partition_info['clients'].items():\n",
    "            if verbose:\n",
    "                print(f\"\\nğŸ“ è¨“ç·´ {client_id}...\")\n",
    "                print(f\"   è³‡æ–™è¦æ¨¡: {client_data['n_samples']} æ¨£æœ¬ Ã— {client_data['n_features']} ç‰¹å¾µ\")\n",
    "                if client_data.get('feature_overlap'):\n",
    "                    print(f\"   ğŸ”— é‡ç–Šç‰¹å¾µ: {len(client_data['feature_overlap'])} å€‹ {client_data['feature_overlap']}\")\n",
    "                \n",
    "            # è¨“ç·´å®¢æˆ¶ç«¯æ¨¡å‹\n",
    "            client_result = self.train_client(\n",
    "                client_id, client_data, X_processed, epochs, verbose=verbose\n",
    "            )\n",
    "            \n",
    "            # å„²å­˜çµæœ\n",
    "            self.client_models[client_id] = client_result['model']\n",
    "            self.client_domains[client_id] = client_result['domains']\n",
    "            \n",
    "            results[client_id] = {\n",
    "                'train_accuracy': client_result['train_accuracy'],\n",
    "                'train_f1': client_result['train_f1'],\n",
    "                'training_time': client_result['training_time'],\n",
    "                'n_samples': client_data['n_samples'],\n",
    "                'n_features': client_data['n_features'],\n",
    "                'feature_overlap': client_data.get('feature_overlap', []),\n",
    "                'config': client_result['config'],\n",
    "                'domains_count': len(client_result['domains'])\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"   ğŸ¯ {client_id} è¨“ç·´å®Œæˆ\")\n",
    "        \n",
    "        # è¨ˆç®—æ•´é«”çµ±è¨ˆ\n",
    "        total_samples = sum(r['n_samples'] for r in results.values())\n",
    "        weighted_accuracy = sum(\n",
    "            r['train_accuracy'] * r['n_samples'] for r in results.values()\n",
    "        ) / total_samples\n",
    "        \n",
    "        weighted_f1 = sum(\n",
    "            r['train_f1'] * r['n_samples'] for r in results.values()\n",
    "        ) / total_samples\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nğŸ“Š {self.partition_info['type']} è¯é‚¦å­¸ç¿’å®Œæˆï¼\")\n",
    "            print(f\"   â±ï¸  ç¸½è¨“ç·´æ™‚é–“: {total_time:.2f} ç§’\")\n",
    "            print(f\"   ğŸ¯ åŠ æ¬Šå¹³å‡è¨“ç·´æº–ç¢ºç‡: {weighted_accuracy:.3f}\")\n",
    "            print(f\"   ğŸ“ˆ åŠ æ¬Šå¹³å‡ F1 åˆ†æ•¸: {weighted_f1:.3f}\")\n",
    "            print(f\"   ğŸ¢ åƒèˆ‡å®¢æˆ¶ç«¯: {len(results)} å€‹\")\n",
    "            print(f\"   ğŸ“Š ç¸½æ¨£æœ¬æ•¸: {total_samples}\")\n",
    "        \n",
    "        return {\n",
    "            'type': self.partition_info['type'],\n",
    "            'client_results': results,\n",
    "            'weighted_accuracy': weighted_accuracy,\n",
    "            'weighted_f1': weighted_f1,\n",
    "            'total_training_time': total_time,\n",
    "            'total_samples': total_samples,\n",
    "            'num_clients': len(results)\n",
    "        }\n",
    "    \n",
    "    def evaluate_on_test(self, X_test, y_test, test_feature_names) -> Dict:\n",
    "        \"\"\"åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°è¯é‚¦æ¨¡å‹\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°è¯é‚¦ EiNet æ¨¡å‹...\")\n",
    "        \n",
    "        client_evaluations = {}\n",
    "        predictions_ensemble = []\n",
    "        probabilities_ensemble = []\n",
    "        \n",
    "        for client_id, model in self.client_models.items():\n",
    "            client_data = self.partition_info['clients'][client_id]\n",
    "            \n",
    "            # æ‰¾åˆ°å®¢æˆ¶ç«¯ç‰¹å¾µåœ¨æ¸¬è©¦é›†ä¸­çš„å°æ‡‰ç´¢å¼•\n",
    "            client_feature_indices = []\n",
    "            for feature in client_data['features']:\n",
    "                if feature in test_feature_names:\n",
    "                    client_feature_indices.append(test_feature_names.index(feature))\n",
    "            \n",
    "            if len(client_feature_indices) == 0:\n",
    "                print(f\"   âš ï¸  {client_id}: æ²’æœ‰å°æ‡‰çš„æ¸¬è©¦ç‰¹å¾µ\")\n",
    "                continue\n",
    "                \n",
    "            # æå–å®¢æˆ¶ç«¯å°æ‡‰çš„æ¸¬è©¦ç‰¹å¾µ\n",
    "            X_test_client = X_test[:, client_feature_indices]\n",
    "            \n",
    "            # æŒ‰ç…§ simple-einet é¢¨æ ¼ reshape\n",
    "            X_test_client_reshaped = torch.tensor(X_test_client).unsqueeze(1)\n",
    "            \n",
    "            # é æ¸¬\n",
    "            try:\n",
    "                acc = accuracy(model, X_test_client_reshaped, y_test)\n",
    "                fscore = f1(model, X_test_client_reshaped, torch.from_numpy(y_test))\n",
    "                \n",
    "                probs = torch.exp(model(X_test_client_reshaped))\n",
    "                predictions = probs.argmax(dim=-1)  \n",
    "                \n",
    "                client_evaluations[client_id] = {\n",
    "                    'accuracy': acc,\n",
    "                    'f1_score': fscore,\n",
    "                    'n_test_features': len(client_feature_indices),\n",
    "                    'predictions': predictions,\n",
    "                }\n",
    "                \n",
    "                predictions_ensemble.append(predictions.detach().numpy())\n",
    "                probabilities_ensemble.append(probs.detach().numpy())\n",
    "                \n",
    "                print(f\"   {client_id}: æº–ç¢ºç‡ {acc:.3f}, F1 {fscore:.3f} ({len(client_feature_indices)} ç‰¹å¾µ)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ {client_id}: è©•ä¼°å¤±æ•— - {str(e)}\")\n",
    "        \n",
    "        # é›†æˆé æ¸¬ï¼ˆç°¡å–®æŠ•ç¥¨å’Œå¹³å‡æ©Ÿç‡ï¼‰\n",
    "        if predictions_ensemble and probabilities_ensemble:\n",
    "            # æŠ•ç¥¨é›†æˆ\n",
    "            predictions_array = np.array(predictions_ensemble)\n",
    "            ensemble_predictions_vote = np.apply_along_axis(\n",
    "                lambda x: np.bincount(x).argmax(), axis=0, arr=predictions_array\n",
    "            )\n",
    "            \n",
    "            # æ©Ÿç‡å¹³å‡é›†æˆ\n",
    "            ensemble_probabilities = np.mean(probabilities_ensemble, axis=0)\n",
    "            ensemble_predictions_prob = np.argmax(ensemble_probabilities, axis=1)\n",
    "            \n",
    "            vote_accuracy = accuracy_score(y_test, ensemble_predictions_vote)\n",
    "            vote_f1 = f1_score(y_test, ensemble_predictions_vote, average='weighted')\n",
    "            # \n",
    "            prob_accuracy = accuracy_score(y_test, ensemble_predictions_prob)\n",
    "            prob_f1 = f1_score(y_test, ensemble_predictions_prob, average='weighted')\n",
    "            \n",
    "            # é¸æ“‡æ›´å¥½çš„é›†æˆæ–¹æ³•\n",
    "            if prob_accuracy >= vote_accuracy:\n",
    "                ensemble_accuracy = prob_accuracy\n",
    "                ensemble_f1 = prob_f1\n",
    "                ensemble_predictions = ensemble_predictions_prob\n",
    "                ensemble_method = \"æ©Ÿç‡å¹³å‡\"\n",
    "            else:\n",
    "                ensemble_accuracy = vote_accuracy\n",
    "                ensemble_f1 = vote_f1\n",
    "                ensemble_predictions = ensemble_predictions_vote\n",
    "                ensemble_method = \"æŠ•ç¥¨\"\n",
    "                \n",
    "        else:\n",
    "            ensemble_accuracy = 0.0\n",
    "            ensemble_f1 = 0.0\n",
    "            ensemble_predictions = None\n",
    "            ensemble_method = \"ç„¡\"\n",
    "        \n",
    "        print(f\"\\nğŸ¯ é›†æˆçµæœ ({ensemble_method}):\")\n",
    "        print(f\"   æº–ç¢ºç‡: {ensemble_accuracy:.3f}\")\n",
    "        print(f\"   F1 åˆ†æ•¸: {ensemble_f1:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'client_evaluations': client_evaluations,\n",
    "            'ensemble_accuracy': ensemble_accuracy,\n",
    "            'ensemble_f1': ensemble_f1,\n",
    "            'ensemble_predictions': ensemble_predictions,\n",
    "            'ensemble_method': ensemble_method\n",
    "        }\n"
   ],
   "id": "37b367d3a1b8bdcb",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:14:26.684958Z",
     "start_time": "2025-08-28T19:14:26.682945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "partitioner = FederatedDataPartitionerRobust(\n",
    "    X=X_train_tensor, \n",
    "    y=y_train_tensor,\n",
    "    feature_names=X_processed.columns.tolist(),\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features\n",
    ")"
   ],
   "id": "3e7199f1de972d22",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:14:32.010435Z",
     "start_time": "2025-08-28T19:14:26.899739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# å¯¦é©— 1: æ°´å¹³è¯é‚¦å­¸ç¿’ - Simple-EiNet é¢¨æ ¼\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”µ å¯¦é©— 1: æ°´å¹³è¯é‚¦å­¸ç¿’ (ä½¿ç”¨ Simple-EiNet API)\")\n",
    "print(\"ç›¸åŒç‰¹å¾µï¼Œä¸åŒæ¨£æœ¬\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "horizontal_partition = partitioner.horizontal_partition(num_clients=3, random_state=42)\n",
    "horizontal_trainer = FederatedEiNetTrainer(horizontal_partition)\n",
    "horizontal_results = horizontal_trainer.train_federated_learning(\n",
    "    X_processed, epochs=5, verbose=True\n",
    ")\n",
    "\n",
    "# åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°\n",
    "horizontal_eval = horizontal_trainer.evaluate_on_test(\n",
    "    X_test, y_test, X_processed.columns.tolist()\n",
    ")"
   ],
   "id": "4128c3494ca9045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ”µ å¯¦é©— 1: æ°´å¹³è¯é‚¦å­¸ç¿’ (ä½¿ç”¨ Simple-EiNet API)\n",
      "ç›¸åŒç‰¹å¾µï¼Œä¸åŒæ¨£æœ¬\n",
      "============================================================\n",
      "ğŸ”„ åŸ·è¡Œæ°´å¹³åˆ†å‰²ï¼Œåˆ†æˆ 3 å€‹å®¢æˆ¶ç«¯...\n",
      "  å®¢æˆ¶ç«¯ 0: 10099 æ¨£æœ¬, 14 ç‰¹å¾µ\n",
      "  å®¢æˆ¶ç«¯ 1: 10099 æ¨£æœ¬, 14 ç‰¹å¾µ\n",
      "  å®¢æˆ¶ç«¯ 2: 10100 æ¨£æœ¬, 14 ç‰¹å¾µ\n",
      "\n",
      "ğŸš€ é–‹å§‹ horizontal è¯é‚¦å­¸ç¿’è¨“ç·´...\n",
      "è¨“ç·´åƒæ•¸: epochs=5\n",
      "\n",
      "ğŸ“ è¨“ç·´ client_0...\n",
      "   è³‡æ–™è¦æ¨¡: 10099 æ¨£æœ¬ Ã— 14 ç‰¹å¾µ\n",
      "   ğŸ”— é‡ç–Šç‰¹å¾µ: 14 å€‹ ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing PiecewiseLinear Leaf Layer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ“Š æ¨¡å‹é…ç½®: depth=2, sums=12, leaves=12\n",
      "    ğŸ”§ ç‰¹å¾µåŸŸ: 14 å€‹ domain\n",
      "Epoch:  5, Loss: 0.7386, Train Acc: 39.94%, F1: 39.77%\n",
      "    âœ… è¨“ç·´æº–ç¢ºç‡: 39.945\n",
      "    ğŸ“ˆ è¨“ç·´ F1 åˆ†æ•¸: 39.771\n",
      "    â±ï¸  è¨“ç·´æ™‚é–“: 1.039 ç§’\n",
      "   ğŸ¯ client_0 è¨“ç·´å®Œæˆ\n",
      "\n",
      "ğŸ“ è¨“ç·´ client_1...\n",
      "   è³‡æ–™è¦æ¨¡: 10099 æ¨£æœ¬ Ã— 14 ç‰¹å¾µ\n",
      "   ğŸ”— é‡ç–Šç‰¹å¾µ: 14 å€‹ ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing PiecewiseLinear Leaf Layer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 12.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ“Š æ¨¡å‹é…ç½®: depth=2, sums=12, leaves=12\n",
      "    ğŸ”§ ç‰¹å¾µåŸŸ: 14 å€‹ domain\n",
      "Epoch:  5, Loss: 0.7369, Train Acc: 33.70%, F1: 33.70%\n",
      "    âœ… è¨“ç·´æº–ç¢ºç‡: 33.696\n",
      "    ğŸ“ˆ è¨“ç·´ F1 åˆ†æ•¸: 33.696\n",
      "    â±ï¸  è¨“ç·´æ™‚é–“: 0.764 ç§’\n",
      "   ğŸ¯ client_1 è¨“ç·´å®Œæˆ\n",
      "\n",
      "ğŸ“ è¨“ç·´ client_2...\n",
      "   è³‡æ–™è¦æ¨¡: 10100 æ¨£æœ¬ Ã— 14 ç‰¹å¾µ\n",
      "   ğŸ”— é‡ç–Šç‰¹å¾µ: 14 å€‹ ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing PiecewiseLinear Leaf Layer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ğŸ“Š æ¨¡å‹é…ç½®: depth=2, sums=12, leaves=12\n",
      "    ğŸ”§ ç‰¹å¾µåŸŸ: 14 å€‹ domain\n",
      "Epoch:  5, Loss: 0.6639, Train Acc: 72.44%, F1: 56.13%\n",
      "    âœ… è¨“ç·´æº–ç¢ºç‡: 72.436\n",
      "    ğŸ“ˆ è¨“ç·´ F1 åˆ†æ•¸: 56.126\n",
      "    â±ï¸  è¨“ç·´æ™‚é–“: 0.767 ç§’\n",
      "   ğŸ¯ client_2 è¨“ç·´å®Œæˆ\n",
      "\n",
      "ğŸ“Š horizontal è¯é‚¦å­¸ç¿’å®Œæˆï¼\n",
      "   â±ï¸  ç¸½è¨“ç·´æ™‚é–“: 3.91 ç§’\n",
      "   ğŸ¯ åŠ æ¬Šå¹³å‡è¨“ç·´æº–ç¢ºç‡: 48.693\n",
      "   ğŸ“ˆ åŠ æ¬Šå¹³å‡ F1 åˆ†æ•¸: 43.198\n",
      "   ğŸ¢ åƒèˆ‡å®¢æˆ¶ç«¯: 3 å€‹\n",
      "   ğŸ“Š ç¸½æ¨£æœ¬æ•¸: 30298\n",
      "\n",
      "ğŸ“‹ åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°è¯é‚¦ EiNet æ¨¡å‹...\n",
      "   client_0: æº–ç¢ºç‡ 46.187, F1 45.285 (14 ç‰¹å¾µ)\n",
      "   client_1: æº–ç¢ºç‡ 32.511, F1 32.490 (14 ç‰¹å¾µ)\n",
      "   client_2: æº–ç¢ºç‡ 73.935, F1 57.164 (14 ç‰¹å¾µ)\n",
      "\n",
      "ğŸ¯ é›†æˆçµæœ (æŠ•ç¥¨):\n",
      "   æº–ç¢ºç‡: 0.518\n",
      "   F1 åˆ†æ•¸: 0.551\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:14:32.012703Z",
     "start_time": "2025-08-28T19:14:32.011487Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "333830b854d0cdb1",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "506e7d9d88ed2a5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
